# Big Data Services

Characteristics of Big data in traditional databases

- massive amounts of data too expensive to store
- no flexibility to store unstructured data
- no support for realtime data
- lack support for Petabyte-scale data volumes

Solution: _Apache Hadoop_ and _NoSQL_ -> but are difficult to deploy, manage in on-premise environments

Big data use cases:

- make better decisions
  - increase revenue
  - get and retain customers
- use with machine learning

## BigQuery

Data warehouse for petabyte scale analysis with built-in machine learning capabilities

Overview:

- ingest data: streaming, bulk loading, batch ingest
  - also from outside Google Cloud
  - streaming provides real time analytics -> can use pub/sub and dataflow for streaming
- real-time analytics
- automatic HA
- automatic backup and restore
- compatible with SQL queries
- integration with Apache big data ecosystem
- access data:
  - web UI
  - `bq` CLI tool
  - apis
- IAM access control and data encryption
- billing: GB stored and queries

## Pub/Sub

Real-time messaging service

- send and receive messages between applications
  - publish to a topic
  - messages held in a message queue
  - subscribe to a topic

Use cases:

- balancing large tasks
- distributing event notifications
- real-time data streaming

## Composer

Workflow orchestration service built on open source `Apache Airflow`

- author, schedule and monitor workflows

Workflow: tasks for ingesting, transforming, analyzing, utilizing data

- DAGs -> collection of tasks that need to be run
- to run a workflow a specific environment is needed -> composer deploys it in GKE

## Dataflow

Serverless processing service for executing `Apache Beam`(open source) pipelines for batch and real time data streaming

- Apache Beam SDK build a pipeline -> dataflow executes that pipeline taking care of all the low-level details
- billing: per-second pricing -> charged for when processing data

Pipeline:

- reads data from a source -> generates a PCollection
- PCollection transformed
- writes transformed data

![Pipeline](ch12.1-big-data-services.dataflow-pipeline.png)

Dataflow Job -> pipeline + other stages

Example:

![Dataflow Job](ch12.1-big-data-services.dataflow-job.png)

## Dataproc

Managed Spark and Hadoop service

- integrated with other Google Cloud services
- billing: 0.01$/vCPU/hour

## Cloud Datalab

Interactive tool to explore, analyze, visualize and create machine learning models from data

- runs on compute engine instance inside a container
- uses jupyter notebooks

## Cloud Dataprep

Serverless for visually exploring, clearing and preparing structured/unstructured data for analysis

- detects schemas, data types, possible joins and anomalies

![Dataprep pipeline](ch12.1-big-data-services.dataprep-pipeline.png)

## Cloud Data Fusion

Code-free and code-friendly service for building and managing ETL/ELT pipelines.
